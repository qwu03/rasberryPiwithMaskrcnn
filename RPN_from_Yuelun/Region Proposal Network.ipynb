{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.layers as KL\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(KL.BatchNormalization):\n",
    "    def call(self, inputs, training=None):\n",
    "        return super(self.__class__, self).call(inputs, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def building_block(filters, block):\n",
    "    if block !=0 :\n",
    "        stride = 1\n",
    "    else:\n",
    "        stride = 2\n",
    "    def f(x):\n",
    "        y = KL.Conv2D(filters, (1,1), strides=stride)(x)\n",
    "        y = BatchNorm(axis=3)(y)\n",
    "        y = KL.Activation(\"relu\")(y)\n",
    "        \n",
    "        y = KL.Conv2D(filters, (3,3), padding=\"same\")(y)\n",
    "        y = BatchNorm(axis=3)(y)\n",
    "        y = KL.Activation(\"relu\")(y)\n",
    "        \n",
    "        y = KL.Conv2D(4*filters, (1,1))(y)\n",
    "        y = BatchNorm(axis=3)(y)\n",
    "        \n",
    "        if block == 0:\n",
    "            shorcut = KL.Conv2D(4*filters, (1,1), strides=stride)(x)\n",
    "            shorcut = BatchNorm(axis=3)(shorcut)\n",
    "        else:\n",
    "            shorcut = x\n",
    "        y = KL.Add()([y, shorcut])\n",
    "        y = KL.Activation(\"relu\")(y)\n",
    "        return y\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resNet_featureExtractor(inputs):\n",
    "    x = KL.Conv2D(64, (3,3), padding=\"same\")(inputs)\n",
    "    x = BatchNorm(axis=3)(x)\n",
    "    x = KL.Activation(\"relu\")(x)\n",
    "    \n",
    "    filters = 64\n",
    "    blocks = [6, 6, 6]\n",
    "    for i, block_num in enumerate(blocks):\n",
    "        for block_id in range(block_num):\n",
    "            x = building_block(filters, block_id)(x)\n",
    "        filters *= 2\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rpn_net(inputs, k):\n",
    "    shared_map = KL.Conv2D(256, (3,3), padding=\"same\")(inputs)\n",
    "    shared_map = KL.Activation(\"linear\")(shared_map)\n",
    "    rpn_class = KL.Conv2D(2*k, (1,1))(shared_map)\n",
    "    rpn_class = KL.Lambda(lambda x: tf.reshape(x, [tf.shape(x)[0],-1,2]))(rpn_class)\n",
    "    rpn_class = KL.Activation(\"linear\")(rpn_class)\n",
    "    rpn_prob = KL.Activation(\"softmax\")(rpn_class)\n",
    "    \n",
    "    y = KL.Conv2D(4*k, (1,1))(shared_map)\n",
    "    y = KL.Activation(\"linear\")(y)\n",
    "    rpn_bbox = KL.Lambda(lambda x: tf.reshape(x, [tf.shape(x)[0],-1,4]))(y)\n",
    "    return rpn_class, rpn_prob, rpn_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rpn_class_loss(rpn_match, rpn_class_logits):\n",
    "    ## rpn_match (None, 576, 1)\n",
    "    ## rpn_class_logits (None, 576, 2)\n",
    "    rpn_match = tf.squeeze(rpn_match, -1)\n",
    "    indices = tf.where(K.not_equal(rpn_match, 0))\n",
    "    anchor_class = K.cast(K.equal(rpn_match, 1), tf.int32)\n",
    "    print(type(anchor_class), anchor_class)\n",
    "    rpn_class_logits = tf.gather_nd(rpn_class_logits, indices)     ### prediction\n",
    "    anchor_class = tf.gather_nd(anchor_class, indices)   ### target\n",
    "    loss = K.sparse_categorical_crossentropy(target=anchor_class, output=rpn_class_logits, from_logits=True)\n",
    "    loss = K.switch(tf.size(loss) > 0 , K.mean(loss), tf.constant(0.0))\n",
    "    return loss\n",
    "\n",
    "def batch_back(x, counts, num_rows):\n",
    "    outputs = []\n",
    "    for i in range(num_rows):\n",
    "        outputs.append(x[i, :counts[i]])\n",
    "    return tf.concat(outputs, axis=0)\n",
    "\n",
    "\n",
    "def rpn_bbox_loss(target_bbox, rpn_match, rpn_bbox):\n",
    "    rpn_match = tf.squeeze(rpn_match, -1)\n",
    "    indices = tf.where(K.equal(rpn_match, 1))\n",
    "    rpn_bbox = tf.gather_nd(rpn_bbox, indices)\n",
    "    batch_counts = K.sum(K.cast(K.equal(rpn_match, 1), tf.int32), axis=1)\n",
    "    target_bbox = batch_back(target_bbox, batch_counts, 20)\n",
    "    diff = K.abs(target_bbox - rpn_bbox)\n",
    "    less_than_one = K.cast(K.less(diff, 1.0), \"float32\")\n",
    "    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n",
    "    loss = K.switch(tf.size(loss) > 0 , K.mean(loss), tf.constant(0.0))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = KL.Input(shape=[64,64,3], dtype=tf.float32)\n",
    "input_bboxes = KL.Input(shape=[None,4], dtype=tf.float32)\n",
    "input_class_ids = KL.Input(shape=[None],dtype=tf.int32)\n",
    "input_rpn_match = KL.Input(shape=[None, 1], dtype=tf.int32)\n",
    "input_rpn_bbox = KL.Input(shape=[None, 4], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "<class 'tensorflow.python.framework.ops.Tensor'> Tensor(\"loss_rpn_match/Cast:0\", shape=(?, ?), dtype=int32)\n",
      "<class 'tensorflow.python.framework.ops.Tensor'> Tensor(\"loss_rpn_match/Cast_2:0\", shape=(?, ?), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "feature_map = resNet_featureExtractor(input_image)\n",
    "rpn_class, rpn_prob, rpn_bbox = rpn_net(feature_map, 9)\n",
    "\n",
    "loss_rpn_match = KL.Lambda(lambda x: rpn_class_loss(*x), name=\"loss_rpn_match\")([input_rpn_match, rpn_class])\n",
    "\n",
    "loss_rpn_bbox = KL.Lambda(lambda x: rpn_bbox_loss(*x), name=\"loss_rpn_bbox\")([input_rpn_bbox, input_rpn_match, rpn_bbox])\n",
    "\n",
    "model = Model([input_image, input_bboxes, input_class_ids, input_rpn_match, input_rpn_bbox],\n",
    "              [rpn_class, rpn_prob, rpn_bbox, loss_rpn_match, loss_rpn_bbox])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Network.summary of <keras.engine.training.Model object at 0xb2d5e6a20>>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "loss_lay1 = model.get_layer(\"loss_rpn_match\").output\n",
    "loss_lay2 = model.get_layer(\"loss_rpn_bbox\").output\n",
    "\n",
    "model.add_loss(tf.reduce_mean(loss_lay1))\n",
    "model.add_loss(tf.reduce_mean(loss_lay2))\n",
    "\n",
    "model.compile(loss=[None]*len(model.output), optimizer=keras.optimizers.SGD(lr=0.00005, momentum=0.9))\n",
    "\n",
    "model.metrics_names.append(\"loss_rpn_match\")\n",
    "model.metrics_tensors.append(tf.reduce_mean(loss_lay1, keep_dims=True))\n",
    "\n",
    "model.metrics_names.append(\"loss_rpn_bbox\")\n",
    "model.metrics_tensors.append(tf.reduce_mean(loss_lay2, keep_dims=True))\n",
    "model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import shapeData as dataSet\n",
    "from config import Config\n",
    "\n",
    "config = Config()\n",
    "dataset = dataSet([64,64], config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_Gen(dataset, num_batch, batch_size, config):\n",
    "    for _ in range(num_batch):\n",
    "        images = []\n",
    "        bboxes = []\n",
    "        class_ids = []\n",
    "        rpn_matchs = []\n",
    "        rpn_bboxes = []\n",
    "        for i in range(batch_size):\n",
    "            image, bbox, class_id, rpn_match, rpn_bbox, _ = data = dataset.load_data()\n",
    "            pad_num = config.max_gt_obj - bbox.shape[0]\n",
    "            pad_box = np.zeros((pad_num, 4))\n",
    "            pad_ids = np.zeros((pad_num, 1))\n",
    "            bbox = np.concatenate([bbox, pad_box], axis=0)\n",
    "            class_id = np.concatenate([class_id, pad_ids], axis=0)\n",
    "        \n",
    "            images.append(image)\n",
    "            bboxes.append(bbox)\n",
    "            class_ids.append(class_id)\n",
    "            rpn_matchs.append(rpn_match)\n",
    "            rpn_bboxes.append(rpn_bbox)\n",
    "        images = np.concatenate(images, 0).reshape(batch_size, config.image_size[0],config.image_size[1] , 3)\n",
    "        bboxes = np.concatenate(bboxes, 0).reshape(batch_size, -1 , 4)\n",
    "        class_ids = np.concatenate(class_ids, 0).reshape(batch_size, -1 )\n",
    "        rpn_matchs = np.concatenate(rpn_matchs, 0).reshape(batch_size, -1 , 1)\n",
    "        rpn_bboxes = np.concatenate(rpn_bboxes, 0).reshape(batch_size, -1 , 4)\n",
    "        yield [images, bboxes, class_ids, rpn_matchs, rpn_bboxes],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataGen = data_Gen(dataset, 35000, 20, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#his = model.fit_generator(dataGen, steps_per_epoch=20, epochs=20)\n",
    "#you can choose to load to trained model. If you wish to train this model, just to uncomment last commond and ignore next block. \n",
    "#For this model you will load, I just trained for 20 epoch. I accidently lost the 500 epoch one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_weights(\"model_material.h5\")\n",
    "model.load_weights(\"model_material.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anchor_refinement(boxes, deltas):\n",
    "    boxes = tf.cast(boxes, tf.float32)\n",
    "    h = boxes[:, 2] - boxes[:, 0]\n",
    "    w = boxes[:, 3] - boxes[:, 1]\n",
    "    center_y = boxes[:, 0] + h / 2\n",
    "    center_x = boxes[:, 1] + w / 2\n",
    "\n",
    "    center_y += deltas[:, 0] * h\n",
    "    center_x += deltas[:, 1] * w\n",
    "    h *= tf.exp(deltas[:, 2])\n",
    "    w *= tf.exp(deltas[:, 3])\n",
    "    \n",
    "    y1 = center_y - h / 2\n",
    "    x1 = center_x - w / 2\n",
    "    y2 = center_y + h / 2\n",
    "    x2 = center_x + w / 2\n",
    "    boxes = tf.stack([y1, x1, y2, x2], axis=1)\n",
    "    return boxes\n",
    "    \n",
    "def boxes_clip(boxes, window):\n",
    "    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n",
    "    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n",
    "    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n",
    "    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n",
    "    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n",
    "    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n",
    "    cliped = tf.concat([y1, x1, y2, x2], axis=1)\n",
    "    cliped.set_shape((cliped.shape[0], 4))\n",
    "    return cliped\n",
    "    \n",
    "def batch_slice(inputs, graph_fn, batch_size):\n",
    "    if not isinstance(inputs, list):\n",
    "        inputs = [inputs]\n",
    "    output = []\n",
    "    for i in range(batch_size):\n",
    "        inputs_slice = [x[i] for x in inputs]\n",
    "        output_slice = graph_fn(*inputs_slice)\n",
    "        if not isinstance(output_slice, (list, tuple)):\n",
    "            output_slice = [output_slice]\n",
    "        output.append(output_slice)\n",
    "    output = list(zip(*output))\n",
    "    result = [tf.stack(o, axis=0) for o in output]\n",
    "    if len(result)==1:\n",
    "        result = result[0]\n",
    "    return result\n",
    "\n",
    "import keras.engine as KE\n",
    "\n",
    "class proposal(KE.Layer):\n",
    "    def __init__(self, proposal_count, nms_thresh, anchors, batch_size, config=None, **kwargs):\n",
    "        super(proposal, self).__init__(**kwargs)\n",
    "        self.proposal_count = proposal_count\n",
    "        self.anchors = anchors\n",
    "        self.nms_thresh = nms_thresh\n",
    "        self.batch_size = batch_size\n",
    "        self.config = config\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        probs = inputs[0][:, :, 1]\n",
    "        deltas = inputs[1]\n",
    "        deltas = deltas * np.reshape(self.config.RPN_BBOX_STD_DEV, (1, 1, 4))\n",
    "        prenms_num = min(100, self.anchors.shape[0])\n",
    "        idxs = tf.nn.top_k(probs, prenms_num).indices\n",
    "        \n",
    "        probs = batch_slice([probs, idxs], lambda x,y:tf.gather(x, y), self.batch_size)\n",
    "        deltas = batch_slice([deltas, idxs], lambda x,y:tf.gather(x, y), self.batch_size)\n",
    "        anchors = batch_slice([idxs], lambda x:tf.gather(self.anchors,x), self.batch_size)\n",
    "        refined_boxes = batch_slice([anchors, deltas], lambda x,y:anchor_refinement(x,y), self.batch_size)\n",
    "        H,W = self.config.image_size[:2]\n",
    "        windows = np.array([0,0,H,W]).astype(np.float32)\n",
    "        cliped_boxes = batch_slice([refined_boxes], lambda x:boxes_clip(x, windows), self.batch_size)\n",
    "        normalized_boxes = cliped_boxes / np.array([H,W,H,W])\n",
    "        def nms(normalized_boxes, scores):\n",
    "            idxs_ = tf.image.non_max_suppression(normalized_boxes, scores, self.proposal_count, self.nms_thresh)\n",
    "            box = tf.gather(normalized_boxes, idxs_)\n",
    "            pad_num = tf.maximum(self.proposal_count - tf.shape(normalized_boxes)[0],0)\n",
    "            box = tf.pad(box, [(0,pad_num),(0,0)])\n",
    "            return box\n",
    "        proposal_ = batch_slice([normalized_boxes, probs], nms, self.batch_size)\n",
    "        return proposal_\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.proposal_count, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = next(dataGen)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = test_data[0]\n",
    "bboxes = test_data[1]\n",
    "class_ids = test_data[2]\n",
    "rpn_matchs = test_data[3]\n",
    "rpn_bboxes = test_data[4]\n",
    "\n",
    "\n",
    "rpn_class, rpn_prob, rpn_bbox, _, _ = \\\n",
    "                model.predict([images, bboxes, class_ids, rpn_matchs, rpn_bboxes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_class = tf.convert_to_tensor(rpn_class)\n",
    "rpn_prob = tf.convert_to_tensor(rpn_prob)\n",
    "rpn_bbox = tf.convert_to_tensor(rpn_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "anchors = utils.anchor_gen([8,8], ratios=config.ratios, scales=config.scales, rpn_stride=config.rpn_stride, \n",
    "                           anchor_stride = config.anchor_stride)\n",
    "\n",
    "\n",
    "proposals = proposal(proposal_count=16, nms_thresh=0.7, anchors=anchors, batch_size=20, config=config)([rpn_prob, rpn_bbox])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "proposals_ = sess.run(proposals) * 64\n",
    "#print(proposals.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "ix = random.sample(range(20), 1)[0]\n",
    "print(ix)\n",
    "proposal_ = proposals_[ix]\n",
    "img = images[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "ix = random.sample(range(20), 1)[0]\n",
    "proposal_ = proposals_[ix]\n",
    "img = images[ix]\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(img)\n",
    "axs = plt.gca()\n",
    "\n",
    "for i in range(proposal_.shape[0]):\n",
    "    box = proposal_[i]\n",
    "    rec = patches.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1], facecolor='none', edgecolor='r')\n",
    "    axs.add_patch(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
